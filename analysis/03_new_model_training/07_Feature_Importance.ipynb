{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a83e7e8-5a32-4877-bccf-a1f5398ee37d",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "### NOTE: There is an error raised within this notebook when splitting training and test sets. \n",
    "ValueError: The least populated class in y has only 1 member, which is too few. \n",
    "\n",
    "\n",
    "SHAP values and XGBoost built-in feature importance are two popular techniques for determining feature importance.\n",
    "\n",
    "SHAP values are computed by analyzing the impact of each feature on the model's output when that feature is included or excluded.\n",
    "\n",
    "XGBoost is a gradient-boosting library that includes a built-in feature importance function that ranks features based on how often they are used to split the data in the boosting process. The XGBoost feature importance function takes into account the contribution of each feature to the model's accuracy.\n",
    "\n",
    "Both SHAP values and XGBoost built-in feature importance provide valuable insights into the importance of different features in a dataset. These techniques may produce different rankings of feature importance so it is useful to compare their results to get a more comprehensive understanding of the importance of different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b37d9-2953-4b52-a267-aa37848686d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f435bf7-203f-4fa6-8aa5-8dbcfe89e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from utils import get_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636b9df-3470-438c-a1d5-68c3e5a4a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file and import to df\n",
    "df = get_training_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histogram of damage\n",
    "df.hist(column=\"percent_houses_damaged\", figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist plot after data stratification\n",
    "bins2 = [0, 0.00009, 1, 10, 50, 101]\n",
    "samples_per_bin2, binsP2 = np.histogram(\n",
    "    df[\"percent_houses_damaged\"], bins=bins2\n",
    ")\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"Damage Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.plot(binsP2[1:], samples_per_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the bins' intervalls (first bin means all zeros, second bin means 0 < values <= 1)\n",
    "df[\"percent_houses_damaged\"].value_counts(bins=binsP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zeros from wind_speed\n",
    "df = df[(df[[\"wind_speed\"]] != 0).any(axis=1)]\n",
    "df = df.drop(columns=[\"grid_point_id\", \"typhoon_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist plot after removing rows where windspeed is 0\n",
    "bins2 = [0, 0.00009, 1, 10, 50, 101]\n",
    "samples_per_bin2, binsP2 = np.histogram(\n",
    "    df[\"percent_houses_damaged\"], bins=bins2\n",
    ")\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"Damage Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.plot(binsP2[1:], samples_per_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d63e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples_per_bin2)\n",
    "print(binsP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the bins' intervalls\n",
    "df[\"percent_houses_damaged\"].value_counts(bins=binsP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_index2 = np.digitize(df[\"percent_houses_damaged\"], bins=binsP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_input_strat = bin_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"wind_speed\",\n",
    "    \"track_distance\",\n",
    "    \"total_houses\",\n",
    "    \"rainfall_max_6h\",\n",
    "    \"rainfall_max_24h\",\n",
    "    \"rwi\",\n",
    "    \"mean_slope\",\n",
    "    \"std_slope\",\n",
    "    \"mean_tri\",\n",
    "    \"std_tri\",\n",
    "    \"mean_elev\",\n",
    "    \"coast_length\",\n",
    "    \"with_coast\",\n",
    "    \"urban\",\n",
    "    \"rural\",\n",
    "    \"water\",\n",
    "    \"total_pop\",\n",
    "    # \"percent_houses_damaged_5years\",\n",
    "]\n",
    "\n",
    "# Split X and y from dataframe features\n",
    "X = df[features]\n",
    "display(X.columns)\n",
    "y = df[\"percent_houses_damaged\"]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7613dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "## NOTE: This causes an error.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled,\n",
    "    df[\"percent_houses_damaged\"],\n",
    "    stratify=y_input_strat,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e2e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# XGBoost Reduced Overfitting\n",
    "xgb = XGBRegressor(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=0.8,\n",
    "    colsample_bynode=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=3,\n",
    "    eta=0.01,\n",
    "    importance_type=\"gain\",\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    missing=1,\n",
    "    n_estimators=100,\n",
    "    early_stopping_rounds=10,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"reg:squarederror\",\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=0.8,\n",
    "    verbosity=1,\n",
    "    eval_metric=[\"rmse\", \"logloss\"],\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "eval_set = [(X_test, y_test)]\n",
    "xgb_model = xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4shapely = pd.DataFrame(data=X_train, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_xgb = shap.Explainer(xgb_model, X_train4shapely)\n",
    "shap_values_xgb = explainer_xgb(X_train4shapely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a622223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Barplot\n",
    "plt.title(\"The Bar plot wrt Shap Values\")\n",
    "shap.plots.bar(shap_values_xgb, max_display=25, show=False)\n",
    "plt.gcf().set_size_inches(9, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Beeswarm Plot\n",
    "# plt.gcf().set_size_inches(4, 3)\n",
    "shap.plots.beeswarm(\n",
    "    shap_values_xgb,\n",
    "    max_display=25,\n",
    "    plot_size=0.7,\n",
    "    # order=shap_values_xgb.abs.max(0)#, color=\"shap_red\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost Built-in Feature Importance\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8.0, 7.0)})\n",
    "plt.rcParams.update({\"font.size\": 10})\n",
    "\n",
    "sorted_idx = xgb.feature_importances_.argsort()\n",
    "plt.barh(X.columns[sorted_idx], xgb.feature_importances_[sorted_idx])\n",
    "plt.title(\"Xgboost built-in Feature Importance\")\n",
    "plt.xlabel(\"Feature Importance values\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
