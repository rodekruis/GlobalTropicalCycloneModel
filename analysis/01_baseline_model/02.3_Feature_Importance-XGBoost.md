The estimation of Feature Importance based on a xgboost regression model. 

After removing the features with correlation value higher than 0.99, analyzing on the input data of the model was done to estimate the most important
features with respect to a xgboost model.
The Feature Importance is done according to two different approaches: 1.SHAP values which used to explain how each
feature affects the model and 2.xgboost Built-in Feature Importance(The XGBoost library provides a built-in function to plot features ordered by their importance).

```python
%load_ext jupyter_black
```

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.ensemble import XGBRegressor
import shap

from utils import get_clean_dataset
```

```python
df = get_clean_dataset()
```

```python
#The Old and New set of bins
bins2= [0, 1, 60, 101]
#bins2 = [0, 0.00009, 1, 10, 50, 101]
samples_per_bin2, binsP2 = np.histogram(df['DAM_perc_dmg'], bins=bins2)
plt.xlabel("Damage Values")
plt.ylabel("Frequency")
plt.plot(binsP2[1:],samples_per_bin2)
```

```python
print(samples_per_bin2)
print(binsP2)
```

```python
bin_index2=np.digitize(df['DAM_perc_dmg'], bins=binsP2)
```

```python
y_input_strat=bin_index2
```

```python
#Dropping highly correlated features (correlation value > 0.99) from X data.
features =[
    'HAZ_rainfall_Total', 
    'HAZ_rainfall_max_6h',
    'HAZ_rainfall_max_24h',
    'HAZ_v_max',
    'HAZ_v_max_3',
    'HAZ_dis_track_min',
    'GEN_landslide_per',
    'GEN_stormsurge_per',
    #'GEN_Bu_p_inSSA', 
    #'GEN_Bu_p_LS', 
    'GEN_Red_per_LSbldg',
    'GEN_Or_per_LSblg', 
    'GEN_Yel_per_LSSAb', 
    #'GEN_RED_per_SSAbldg',
    'GEN_OR_per_SSAbldg',
    'GEN_Yellow_per_LSbl',
    'TOP_mean_slope',
    'TOP_mean_elevation_m', 
    'TOP_ruggedness_stdev', 
    #'TOP_mean_ruggedness',
    #'TOP_slope_stdev', 
    'VUL_poverty_perc',
    'GEN_with_coast',
    'GEN_coast_length', 
    'VUL_Housing_Units',
    'VUL_StrongRoof_StrongWall', 
    'VUL_StrongRoof_LightWall',
    'VUL_StrongRoof_SalvageWall', 
    'VUL_LightRoof_StrongWall',
    'VUL_LightRoof_LightWall', 
    'VUL_LightRoof_SalvageWall',
    'VUL_SalvagedRoof_StrongWall',
    'VUL_SalvagedRoof_LightWall',
    'VUL_SalvagedRoof_SalvageWall', 
    'VUL_vulnerable_groups',
    'VUL_pantawid_pamilya_beneficiary'
]

X = df[features]
display(X.columns)
y = df["DAM_perc_dmg"]

scaler = preprocessing.StandardScaler().fit(X)
X_scaled = scaler.transform(X)

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X_scaled,df['DAM_perc_dmg'], stratify=y_input_strat, test_size=0.2, 
                                                    #random_state=42
                                                   ) 


#print(X_train.shape, y_train.shape)
#print(X_test.shape, y_test.shape)

```

```python
# create an xgboost regression model

#XGBoost
xgb = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, gamma=1, reg_lambda=0.1, colsample_bytree=0.8 
                   #,random_state=42
                  )
xgb_model=xgb.fit(X_train, y_train)


#XGBoost ReducedOverfitting
#xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.8,
#                   colsample_bynode=0.8, colsample_bytree=0.8, gamma=3, eta=0.01,
#                   importance_type='gain', learning_rate=0.1, max_delta_step=0,
#                   max_depth=4, min_child_weight=1, missing=1, n_estimators=100, early_stopping_rounds=10,
#                   n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,
#                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
#                   silent=None, subsample=0.8, verbosity=1, eval_metric=["rmse", "logloss"])

    
#eval_set = [(X_test, y_test)]
#xgb_model=xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False)

```

```python
X_train4shapely=pd.DataFrame(data=X_train,columns=features)
```

```python
explainer_xgb = shap.Explainer(xgb_model, X_train4shapely)
shap_values_xgb = explainer_xgb(X_train4shapely)

```

```python
#Showing Barplot

#shap.plots.beeswarm(shap_values_xgb.abs, color="shap_red", max_display=20)
#shap.plots.bar(shap_values_xgb.abs.mean(0), max_display=20)

shap.plots.bar(shap_values_xgb, max_display=20)
```

```python
#Showing Beeswarm Plot
shap.plots.beeswarm(shap_values_xgb, max_display=20, #order=shap_values_xgb.abs.max(0)#, color="shap_red"
                   )
```

```python
#Showing Heatmap 
shap.plots.heatmap(shap_values_xgb[:1000])
```

```python
#Xgboost Built-in Feature Importance

plt.rcParams.update({'figure.figsize': (12.0, 8.0)})
plt.rcParams.update({'font.size': 10})

sorted_idx = xgb.feature_importances_.argsort()
plt.barh(X.columns[sorted_idx], xgb.feature_importances_[sorted_idx])
plt.xlabel("Xgboost Feature Importance")
```

import shap explainer = shap.TreeExplainer(xgb) shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar")

```python
"""
Showing X data illustrates that 5 highly correlated features with the value higher than 0.99 were removed 
before applying feature importance methods on input data.
"""
X
```
