# Assignment of building damage data to grids from municipalities

This notebook that uses the weights from the area and the number of buildings.
The rasterisation of damage data is done in this notebook.

```python
%load_ext jupyter_black
import pandas as pd
import os
from pathlib import Path

pd.set_option("display.float_format", lambda x: "%.5f" % x)
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis/02_new_model_input/02_housing_damage/input/"
)
baseline_input_dir = (
    Path(os.getenv("STORM_DATA_DIR")) / "analysis/01_baseline_model/input/"
)
output_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis/02_new_model_input/02_housing_damage/output/"
)
```

```python
# reading in file with building damage data and adm3 in grid
adm3_perc_ingrid = pd.read_csv(
    input_dir / "Adm3_Perc_inGrid_Weight_Matrix.csv"
)
perc_build_dmg_data = pd.read_csv(
    baseline_input_dir / "combined_input_data.csv"
)
build_dmg_data = pd.read_csv(input_dir / "IMpact_data_philipines_SEP_2021.csv")
phl_build_weights = pd.read_csv(input_dir / "phl_bld_weight_matrix.csv")
phl_build_grid = pd.read_csv(
    input_dir / "phl_hotosm_bld_centroid_grid_count.csv"
)
phl_build_municip = pd.read_csv(
    input_dir / "phl_hotosm_bld_centroid_municip_count.csv"
)
```

```python
# removing duplicates from the building damage data
# duplicates removed and kep only the first value for each
# municipality, typhoon and year
build_dmg_data.drop("Id", axis=1, inplace=True)
build_dmg_data.drop_duplicates(
    subset=["pcode", "typhoon", "Year"], inplace=True
)
```

```python
build_dmg_data[build_dmg_data["pcode"] == "PH025012000"]
```

We will come back to this
as it is not clear why there are multiple rows for the same municipalities
recorded for the same typhoon and Year in the building damage data set.
Different total values could be accumulations.
For now, treating them as separate values.

```python
## aggregating these values
## To confirm whether they are additional data or cumulative data in the case
# where the total values are different for same typhoons.

# build_dmg_data_grouped = (
#    build_dmg_data.groupby(["pcode", "typhoon", "Year"]).sum().reset_index()
# )
# build_dmg_data_grouped[build_dmg_data_grouped["pcode"] == "PH025012000"]
```

```python
all(
    item in list(phl_build_municip["ADM3_PCODE"])
    for item in list(build_dmg_data["pcode"])
)
# Not all municipalities are in the damage data set.
# Not all municipalities in the building damage data can be found in
# the admin 3 shapefile.
# removing those buildings with incorrect pcode in them
build_dmg_data_grouped = build_dmg_data[
    build_dmg_data["pcode"].isin(list(phl_build_municip["ADM3_PCODE"]))
]
build_dmg_data_grouped["Totally"].sum()
```

## Using Area of Municipality

```python
## Section describing the merging of the north and south buildings from
# HOTOSM shapefile
merged_df_points = phl_build_grid.copy()
merged_df_points = pd.merge(
    merged_df_points,
    phl_build_grid.drop("numbuildings", axis=1),
    on="id",
    suffixes=(None, "_y"),
)

merged_df_points["numbuildings"].describe()
```

```python
merged_df_points_overlap = merged_df_points.loc[
    merged_df_points["Centroid"].isin(adm3_perc_ingrid["Centroid"])
]
```

```python
### Section describing the computation of the building damage percentage
# combining building damage data and grid information
merged_total_damage_df = adm3_perc_ingrid.merge(
    build_dmg_data_grouped,
    left_on="ADM3_PCODE",
    right_on="pcode",
    how="left",
)
```

```python
# computing % in each grid
# totally damaged
merged_total_damage_df["Totally_Damaged_bygrid"] = (
    merged_total_damage_df["Municipality Completeness"]
    * merged_total_damage_df["Totally"]
)
```

```python
merged_total_damage_df[
    (merged_total_damage_df["pcode"] == "PH025012000")
    & (merged_total_damage_df["typhoon"] == "Mangkhut")
]
```

```python
test_df = (
    merged_total_damage_df[["ADM3_PCODE", "Totally_Damaged_bygrid"]]
    .groupby("ADM3_PCODE")
    .sum()
    .reset_index()
    .merge(
        build_dmg_data_grouped[["pcode", "Totally"]]
        .groupby("pcode")
        .sum()
        .reset_index(),
        left_on="ADM3_PCODE",
        right_on="pcode",
        how="left",
    )
)
test_df["Diff"] = test_df["Totally_Damaged_bygrid"] - test_df["Totally"]
test_df["Diff"].describe()
```

A small difference between the number of totally damaged values.
This can be explained by the weighting using the area overlap.
There may be some rounding when computing the area.

```python
# computing percentage damage
# merging with building damage data
merged_perc_damage_df = merged_df_points.merge(
    merged_total_damage_df, on="id", how="right", suffixes=(None, "_y")
)
merged_perc_damage_df[
    [
        "id",
        "Centroid",
        "ADM3_PCODE",
        "ADM3_EN",
        "typhoon",
        "Year",
        "Municipality Completeness",
        "Totally",
        "numbuildings",
        "Totally_Damaged_bygrid",
    ]
].sort_values(["Totally_Damaged_bygrid"], ascending=False)
```

We changed the approach to use number of buildings in a municipality instead.
Since we are using HOTOSM data, which is differently sourced from the damage data,
some municipalities and grids have more damaged buildings than total number of buildings.
TODO: Find new building dataset that is more accurate.

```python
merged_perc_damage_dfout = (
    merged_perc_damage_df[
        [
            "id",
            "Centroid",
            "ADM3_PCODE",
            "ADM3_EN",
            "typhoon",
            "Year",
            "numbuildings",
            "Totally_Damaged_bygrid",
        ]
    ]
    .groupby(["id", "Centroid", "typhoon", "Year"])
    .sum(numeric_only=True)
    .reset_index()
)
# computing the percentage damage
merged_perc_damage_dfout["Totally_Damaged_Perc_bygrid"] = (
    merged_perc_damage_dfout["Totally_Damaged_bygrid"]
    / merged_perc_damage_dfout["numbuildings"]
)
merged_perc_damage_dfout.sort_values(
    ["Totally_Damaged_Perc_bygrid"], ascending=False
)
```

```python
merged_perc_damage_dfout["Totally_Damaged_bygrid"].sum()
```

```python
# writing output to CSV file
# to write to csv file, group first by grid centroid
merged_perc_damage_dfout.to_csv(
    output_dir / "building_damage_bygrid_using_area.csv", index=False
)
```

## Using Number of Buildings

Merging all dataframes, one with number of buildings in municipality,
one with number of damaged buildings in municipality and
the last with weights for each grid and municipality.

```python
phl_bld_all_merged_df = phl_build_municip.merge(
    build_dmg_data_grouped,
    left_on="ADM3_PCODE",
    right_on="pcode",
    how="left",
    suffixes=(None, "_y"),
).merge(phl_build_weights, on="ADM3_PCODE", how="left", suffixes=(None, "_y"))
```

```python
phl_bld_all_merged_df[phl_bld_all_merged_df["ADM3_PCODE"] == "PH175301000"]
```

```python
phl_build_weights.groupby("ADM3_PCODE")["weight"].sum().describe()
```

```python
phl_bld_all_merged_df.groupby(["pcode", "typhoon", "Year"]).first()[
    "Totally"
].sum()
```

```python
phl_bld_all_merged_df.groupby(["pcode", "typhoon", "Year"]).first()[
    "numbuildings"
].sum()
```

```python
phl_build_municip["numbuildings"].sum()
```

```python
phl_bld_all_merged_df["numbuildings_bygrid"] = (
    phl_bld_all_merged_df["weight"] * phl_bld_all_merged_df["numbuildings"]
)
phl_bld_all_merged_df["Totally_Damaged_bygrid"] = (
    phl_bld_all_merged_df["weight"] * phl_bld_all_merged_df["Totally"]
)
phl_bld_all_merged_df["Totally_Damaged_bygrid"] = phl_bld_all_merged_df[
    "Totally_Damaged_bygrid"
].fillna(0)
phl_bld_all_merged_df[
    [
        "id",
        "Centroid",
        "ADM3_PCODE",
        "typhoon",
        "Year",
        "weight",
        "Totally",
        "numbuildings",
        "numbuildings_bygrid",
        "Totally_Damaged_bygrid",
    ]
].sort_values(["Totally_Damaged_bygrid"], ascending=False)
```

```python
phl_bld_all_merged_df[phl_bld_all_merged_df["ADM3_PCODE"] == "PH025012000"][
    [
        "id",
        "Centroid",
        "ADM3_PCODE",
        "typhoon",
        "Year",
        "weight",
        "Totally",
        "numbuildings",
        "numbuildings_bygrid",
        "Totally_Damaged_bygrid",
    ]
]
```

```python
phl_bld_all_merged_df["Totally_Damaged_bygrid"].sum()
```

```python
phl_bld_all_merged_df["numbuildings_bygrid"].sum()
```

```python
phl_bld_all_merged_dfout = phl_bld_all_merged_df[
    [
        "id",
        "Centroid",
        "typhoon",
        "Year",
        "weight",
        "numbuildings_bygrid",
        "Totally_Damaged_bygrid",
    ]
]
phl_bld_all_merged_dfout = (
    phl_bld_all_merged_dfout.groupby(["id", "Centroid", "typhoon", "Year"])
    .sum()
    .reset_index()
)

phl_bld_all_merged_dfout["Totally_Damaged_Perc_bygrid"] = (
    phl_bld_all_merged_dfout["Totally_Damaged_bygrid"]
    / phl_bld_all_merged_dfout["numbuildings_bygrid"]
)
phl_bld_all_merged_dfout.sort_values(
    ["Totally_Damaged_Perc_bygrid"], ascending=False
)
```

```python
phl_bld_all_merged_dfout["numbuildings_bygrid"].sum()
```

```python
phl_bld_all_merged_dfout[phl_bld_all_merged_dfout["Centroid"] == "126.6E_7.3N"]
```

```python
phl_bld_all_merged_dfout.groupby(["Centroid"])[
    "numbuildings_bygrid"
].sum().reset_index()
```

```python
phl_bld_all_merged_dfout["Totally_Damaged_bygrid"].sum()
```

```python
phl_bld_all_merged_dfout["Totally_Damaged_Perc_bygrid"].describe()
```

```python
# writing output to CSV file
# to write to csv file, group first by grid centroid
phl_bld_all_merged_dfout.to_csv(
    output_dir / "building_damage_bygrid.csv", index=False
)
```
