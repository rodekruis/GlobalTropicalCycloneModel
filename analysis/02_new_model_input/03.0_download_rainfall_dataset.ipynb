{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that downloads GPM rainfall data done per typhoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "input_dir = (\n",
    "    Path(os.getenv(\"STORM_DATA_DIR\"))\n",
    "    / \"analysis/02_new_model_input/03_rainfall/input\"\n",
    ")\n",
    "# Importing local libraries\n",
    "typhoon_metadata = pd.read_csv(input_dir / \"metadata_typhoons.csv\")\n",
    "# Setting path to save the GPM data\n",
    "gpm_file_name = \"gpm_data/rainfall_data/output_hhr/\"\n",
    "gpm_folder_path = Path(input_dir, gpm_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create an account for downloading the data\n",
    "# follow the instructions here: https://registration.pps.eosdis.nasa.gov/registration/\n",
    "# Change the user name and provide the password in the code\n",
    "user_name = \"pauline.ndirangu@un.org\"\n",
    "password = getpass.getpass(prompt=\"Password: \", stream=None)\n",
    "baseurl = \"https://arthurhouhttps.pps.eosdis.nasa.gov/pub/gpmdata\"\n",
    "# y=2021\n",
    "# m=4\n",
    "# d=13\n",
    "# file=f\"/{y}/0{m}/{d}/gis\"\n",
    "# url=baseurl+file\n",
    "ext = \"tif\"\n",
    "##########################\n",
    "# Setting the number of days prior to the landfall data for which to collect data\n",
    "days_to_landfall = 2\n",
    "\n",
    "# Default = FALSE\n",
    "# IMPORTANT: setting to TRUE means that\n",
    "# all downloaded GPM files will be deleted and re-downloaded\n",
    "delete_folders = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Functions used\n",
    "def date_range(start_date, end_date):\n",
    "    return [\n",
    "        str(start_date + dt.timedelta(days=x))\n",
    "        for x in range((end_date - start_date).days + 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "def list_files(url, user_name, password, ext=\"\"):\n",
    "    try:\n",
    "        page = requests.get(url, auth=(user_name, password)).text\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(\"Error:\\n\", error)\n",
    "    return [\n",
    "        url + \"/\" + node.get(\"href\")\n",
    "        for node in soup.find_all(\"a\")\n",
    "        if node.get(\"href\").endswith(ext)\n",
    "    ]\n",
    "\n",
    "\n",
    "def download_gpm_http(start_date, end_date, download_path, type_imerg):\n",
    "    # Username and password for logging in\n",
    "    # Can create own account on NASA site\n",
    "    # user_name = \"pauline.ndirangu@un.org\"#GPM_USERNAME\n",
    "    base_url = \"\"\n",
    "\n",
    "    # Connection to the site, if pasting in chrome:\n",
    "    # https://arthurhouhttps.pps.eosdis.nasa.gov/\n",
    "    # Directory to where the data is saved\n",
    "    if type_imerg == \"final\":\n",
    "        base_url = baseurl\n",
    "\n",
    "    date_list = date_range(start_date, end_date)\n",
    "    file_list = []\n",
    "\n",
    "    for date in date_list:\n",
    "        print(date)\n",
    "        d, m, y = reversed(date.split(\"-\"))\n",
    "        day_path = download_path / str(y + m + d)\n",
    "\n",
    "        # Make a folder for each day, to save GPM data\n",
    "\n",
    "        os.makedirs(day_path, exist_ok=True)\n",
    "        if type_imerg == \"final\":\n",
    "            data_dir_final = f\"/{y}/{m}/{d}/gis\"\n",
    "            url = base_url + data_dir_final\n",
    "            tiff_files = list_files(\n",
    "                url, user_name=user_name, password=user_name, ext=\"tif\"\n",
    "            )\n",
    "\n",
    "            for tifffile in tiff_files:\n",
    "                file_name = tifffile.split(\"/\")[-1]\n",
    "\n",
    "                file_path = day_path / file_name\n",
    "                file_list.append(file_path)\n",
    "                try:\n",
    "                    r = requests.get(tifffile, auth=(user_name, user_name))\n",
    "                    open(file_path, \"wb\").write(r.content)\n",
    "                except requests.exceptions.RequestException as error:\n",
    "                    print(\"Error:\\n\", error)\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure the dates can be converted to date type\n",
    "for i in range(len(typhoon_metadata)):\n",
    "    typhoon_metadata[\"startdate\"][i] = typhoon_metadata[\"startdate\"][\n",
    "        i\n",
    "    ].replace(\"/\", \"-\")\n",
    "    typhoon_metadata[\"enddate\"][i] = typhoon_metadata[\"enddate\"][i].replace(\n",
    "        \"/\", \"-\"\n",
    "    )\n",
    "    typhoon_metadata[\"landfalldate\"][i] = typhoon_metadata[\"landfalldate\"][\n",
    "        i\n",
    "    ].replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typhoons for which to download rainfall data\n",
    "typhoons = list(typhoon_metadata.typhoon.values)\n",
    "\n",
    "# if there is already rainfall data in the project folder  output_hhr_processed\n",
    "typhoons_with_rainfall_data = [\n",
    "    items.split(\"_\")[0]\n",
    "    for items in os.listdir(str(input_dir) + \"/\" + gpm_file_name)\n",
    "]\n",
    "typhoons_without_rainfall_data = [\n",
    "    items for items in typhoons if items not in typhoons_with_rainfall_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typhoon_metadata_ = typhoon_metadata.set_index(\"typhoon\").to_dict()\n",
    "typhoons_dict = dict()\n",
    "\n",
    "###################################################################\n",
    "### START OF PROCESSING\n",
    "###################################################################\n",
    "\n",
    "for typhoon in typhoons_with_rainfall_data:\n",
    "    case = typhoon\n",
    "    typhoons_dict[case] = {\n",
    "        \"typhoon\": typhoon,\n",
    "        \"dates\": [\n",
    "            dt.datetime.strptime(\n",
    "                typhoon_metadata_[\"landfalldate\"][typhoon], \"%d-%m-%Y\"\n",
    "            ).date()\n",
    "            - dt.timedelta(days=2),\n",
    "            dt.datetime.strptime(\n",
    "                typhoon_metadata_[\"landfalldate\"][typhoon], \"%d-%m-%Y\"\n",
    "            ).date()\n",
    "            + dt.timedelta(days=2),\n",
    "            dt.datetime.strptime(\n",
    "                typhoon_metadata_[\"landfalldate\"][typhoon], \"%d-%m-%Y\"\n",
    "            ).date(),\n",
    "        ],\n",
    "        \"imerg_type\": typhoon_metadata_[\"imerg_type\"][typhoon],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes a long time to download the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in list(typhoons_dict.keys())[0:1]:\n",
    "    typhoon_to_process = typhoons_dict[keys]\n",
    "    typhoon = typhoon_to_process.get(\"typhoon\")\n",
    "\n",
    "    # Start/End date for precipitation data, get from the dictionary\n",
    "    start_date = min(typhoon_to_process.get(\"dates\"))\n",
    "    end_date = max(typhoon_to_process.get(\"dates\"))\n",
    "    print(\"start_date is:\", start_date, \"end date of typhoon is:\", end_date)\n",
    "\n",
    "    # IMERG data type, either \"early\" (6hr), \"late\" (18hr) or \"final\" (4 months),\n",
    "    # see https://pps.gsfc.nasa.gov/Documents/README.GIS.pdf\n",
    "    imerg_type = typhoon_to_process.get(\"imerg_type\")  # \"early\"\n",
    "    print(\"imerg_type:\", imerg_type)\n",
    "\n",
    "    t0 = dt.datetime.now()\n",
    "\n",
    "    # Specify the names to save the GPM data (folder) and the output file\n",
    "    subfolder = typhoon  # + \"/\"\n",
    "    gpm_path = gpm_folder_path / subfolder / \"GPM\"\n",
    "\n",
    "    # Downloading rainfall data\n",
    "    if not imerg_type == \"trmm\":\n",
    "        download_gpm_http(start_date, end_date, gpm_path, imerg_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('globaltyphoon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0bf5227c718a54401bc80004b44f7ad33fb80a867a635817764b403a4b4c0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
