{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that reads in GPM rainfall data and extracts data for cells in the Philippines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "input_dir = (\n",
    "    Path(os.getenv(\"STORM_DATA_DIR\"))\n",
    "    / \"analysis/02_new_model_input/03_rainfall/input\"\n",
    ")\n",
    "\n",
    "# grid\n",
    "grid_name = \"analysis/02_new_model_input/02_housing_damage/output/phl_0.1_degree_grid_land_overlap.gpkg\"\n",
    "grid_file = Path(os.getenv(\"STORM_DATA_DIR\")) / grid_name\n",
    "grid = gpd.read_file(grid_file)\n",
    "\n",
    "# gpm data\n",
    "gpm_file_name = \"gpm_data/rainfall_data/output_hhr/\"\n",
    "gpm_folder_path = Path(input_dir, gpm_file_name)\n",
    "typhoon_list = os.listdir(gpm_folder_path)\n",
    "\n",
    "# outputs\n",
    "processed_output_dir = Path(\n",
    "    input_dir / \"gpm_data/rainfall_data/output_hhr_processed/\"\n",
    ")\n",
    "output_dir = (\n",
    "    Path(os.getenv(\"STORM_DATA_DIR\"))\n",
    "    / \"analysis/02_new_model_input/03_rainfall/output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing raster method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the raster stats method\n",
    "day_list = os.listdir(gpm_folder_path / typhoon_list[1] / \"GPM\")\n",
    "file_list = os.listdir(gpm_folder_path / typhoon_list[1] / \"GPM\" / day_list[0])\n",
    "file = Path(\n",
    "    gpm_folder_path / typhoon_list[1] / \"GPM\" / day_list[0] / file_list[0]\n",
    ")\n",
    "input_raster = rasterio.open(file)\n",
    "array = input_raster.read(1)\n",
    "# checking if crs are the same\n",
    "input_raster.crs == grid.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing stats for 4 adjacent cells\n",
    "summary_stats = zonal_stats(\n",
    "    grid,\n",
    "    array,\n",
    "    stats=[\"min\", \"max\", \"mean\", \"std\"],\n",
    "    nodata=29999,\n",
    "    all_touched=True,\n",
    "    affine=input_raster.transform,\n",
    ")\n",
    "grid_stats = pd.DataFrame(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using OCHA AnticiPy for zonal stats\n",
    "## Install via pip install ocha-anticipy\n",
    "# More information here: https://github.com/OCHA-DAP/ocha-anticipy\n",
    "# https://ocha-anticipy.readthedocs.io/en/latest/introduction.html\n",
    "# Takes longer than zonal stats\n",
    "\n",
    "import ochanticipy\n",
    "import rioxarray as rio\n",
    "\n",
    "# opening raster and clipping it to use only grid extents and extracting stats\n",
    "input_raster = rio.open_rasterio(file)\n",
    "rast_clip = input_raster.rio.clip_box(*grid.total_bounds)\n",
    "grid_df = rast_clip.oap.compute_raster_stats(\n",
    "    gdf=grid, feature_col=\"Centroid\", all_touched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing loop using rio.clip\n",
    "# Takes the longest time out of the 3 methods\n",
    "grid_vals = pd.DataFrame(\n",
    "    {\"id\": grid[\"id\"], \"Centroid\": grid[\"Centroid\"], \"mean\": None, \"max\": None}\n",
    ")\n",
    "for grd in grid.Centroid:\n",
    "    grd_sel = grid[grid.Centroid == grd]\n",
    "    grid_mean = rast_clip.rio.clip(grd_sel[\"geometry\"], all_touched=True)\n",
    "    grid_vals.loc[grd_sel.index.values, [\"mean\"]] = grid_mean.mean().values\n",
    "    grid_vals.loc[grd_sel.index.values, [\"max\"]] = grid_mean.max().values\n",
    "\n",
    "# Will proceed with raster stats as it seems to be the fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over typhoons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes a very long time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up loop for running through all typhoons\n",
    "# extracting the max and mean of the 4 adjacent cells due to shifting to grids\n",
    "stats_list = [\"mean\", \"max\"]\n",
    "for typ in typhoon_list:\n",
    "    day_list = os.listdir(gpm_folder_path / typ / \"GPM\")\n",
    "    day_df = pd.DataFrame()\n",
    "    for day in day_list:\n",
    "        file_list = os.listdir(gpm_folder_path / typ / \"GPM\" / day)\n",
    "        file_df = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            if file.startswith(\"3B-HHR\"):\n",
    "                file_path = Path(gpm_folder_path / typ / \"GPM\" / day / file)\n",
    "                input_raster = rasterio.open(file_path)\n",
    "                array = input_raster.read(1)\n",
    "                summary_stats = zonal_stats(\n",
    "                    grid,\n",
    "                    array,\n",
    "                    stats=stats_list,\n",
    "                    nodata=29999,\n",
    "                    all_touched=True,\n",
    "                    affine=input_raster.transform,\n",
    "                )\n",
    "                grid_stats = pd.DataFrame(summary_stats)\n",
    "                # change values by dividing by 10 to mm/hr\n",
    "                grid_stats[stats_list] /= 10\n",
    "                grid_merged = pd.merge(\n",
    "                    grid.drop([\"geometry\", \"Longitude\", \"Latitude\"], axis=1),\n",
    "                    grid_stats,\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                )\n",
    "                grid_merged[\"start\"] = \"%s%s:%s%s:%s%s\" % (\n",
    "                    *file.split(\"-S\")[1][0:6],\n",
    "                )\n",
    "                grid_merged[\"end\"] = \"%s%s:%s%s:%s%s\" % (\n",
    "                    *file.split(\"-E\")[1][0:6],\n",
    "                )\n",
    "\n",
    "                file_df = pd.concat([file_df, grid_merged], axis=0)\n",
    "        file_df[\"date\"] = str(day)\n",
    "        day_df = pd.concat([day_df, file_df], axis=0)\n",
    "    day_df[\"time\"] = day_df[\"date\"].astype(str) + \"_\" + day_df[\"start\"]\n",
    "    for stats in stats_list:\n",
    "        day_wide = pd.pivot(\n",
    "            day_df,\n",
    "            index=[\"id\", \"Centroid\"],\n",
    "            columns=[\"time\"],\n",
    "            values=[stats],\n",
    "        )\n",
    "        day_wide.columns = day_wide.columns.droplevel(0)\n",
    "        day_wide.reset_index(inplace=True)\n",
    "        day_wide.to_csv(\n",
    "            processed_output_dir / str(typ + \"_gridstats_\" + stats + \".csv\"),\n",
    "            index=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0bf5227c718a54401bc80004b44f7ad33fb80a867a635817764b403a4b4c0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
