# Notebook that downloads GPM rainfall data done per typhoon

```python
%load_ext jupyter_black
```

```python
import getpass
import os
from pathlib import Path


import pandas as pd
import datetime as dt
from bs4 import BeautifulSoup
import requests
```

```python
# Setting directories
input_dir = (
    Path(os.getenv("STORM_DATA_DIR"))
    / "analysis/02_new_model_input/03_rainfall/input"
)
# Setting path to save the GPM data
gpm_file_name = "gpm_data/rainfall_data/output_hhr/"
gpm_folder_path = Path(input_dir, gpm_file_name)
```

```python
# To create an account for downloading the data
# follow the instructions here: https://registration.pps.eosdis.nasa.gov/registration/
# Change the user name and provide the password in the code
USERNAME =  getpass.getpass(prompt="Username: ", stream=None)
PASSWORD = getpass.getpass(prompt="Password: ", stream=None)

# Setting the number of days prior to the landfall data for which to collect data
DAYS_TO_LANDFALL = 2
```

```python
# Load and clean the typhoon metadata
# We really only care about the landfall date
typhoon_metadata = pd.read_csv(input_dir / "metadata_typhoons.csv").set_index(
    "typhoon"
)
for colname in ["startdate", "enddate", "landfalldate"]:
    typhoon_metadata[colname] = pd.to_datetime(
        typhoon_metadata[colname], format="%d/%m/%Y"
    )
typhoon_metadata
```

```python
#%% Functions used
def list_files(url):
    page = requests.get(url, auth=(USERNAME, PASSWORD)).text
    soup = BeautifulSoup(page, "html.parser")
    return [
        url + "/" + node.get("href")
        for node in soup.find_all("a")
        if node.get("href").endswith("tif")
    ]


def download_gpm_http(start_date, end_date, download_path):
    base_url = "https://arthurhouhttps.pps.eosdis.nasa.gov/pub/gpmdata"

    date_list = pd.date_range(start_date, end_date)
    file_list = []

    for date in date_list:
        print(f"Downloading data for date {date}")
        day_path = download_path / date.strftime("%Y%m%d")
        day_path.mkdir(parents=True, exist_ok=True)

        url = f"{base_url}/{date.strftime('%Y/%m/%d')}/gis"
        tiff_files = list_files(url=url)

        for tiff_file in tiff_files:
            file_name = tiff_file.split("/")[-1]

            file_path = day_path / file_name
            file_list.append(file_path)
            r = requests.get(tiff_file, auth=(user_name, user_name))
            open(file_path, "wb").write(r.content)

    return file_list
```

## Download the data

This section is for downloading the data.
It takes a long time to complete.

```python
for typhoon, metadata in typhoon_metadata.iterrows():
    start_date = metadata["landfalldate"] - dt.timedelta(days=DAYS_TO_LANDFALL)
    end_date = metadata["landfalldate"] - dt.timedelta(days=DAYS_TO_LANDFALL)
    print(f"Downloading data for {typhoon} between {start_date} and {end_date}")
    download_gpm_http(start_date=start_date,
                      end_date=end_date,
                      download_path=gpm_folder_path / typhoon / "GPM")
```
